{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/datvodinh10/project-DD/blob/master/main.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Import"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "c:\\Users\\vodin\\project-DD\n"
          ]
        }
      ],
      "source": [
        "%cd .."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "import math\n",
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "bGym2TwrDvSU"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\vodin\\.conda\\envs\\pytorchenv\\lib\\site-packages\\torchvision\\io\\image.py:11: UserWarning: Failed to load image Python extension: Could not find module 'C:\\Users\\vodin\\.conda\\envs\\pytorchenv\\Lib\\site-packages\\torchvision\\image.pyd' (or one of its dependencies). Try using the full path with constructor syntax.\n",
            "  warn(f\"Failed to load image Python extension: {e}\")\n"
          ]
        }
      ],
      "source": [
        "from src.model.trainer import Trainer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "device(type='cuda', index=0)"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "VmszHHmlDvSV"
      },
      "outputs": [],
      "source": [
        "config = {\n",
        "    'transformer':{\n",
        "        'embed_size': 384,      # model's hidden size\n",
        "        'num_heads':8,          # number of heads in MSA\n",
        "        'num_layers':2,         # number of encoder/decoder layer\n",
        "        'max_len': 320,         # max sequence length\n",
        "        'dropout':0.1,          # dropout rate\n",
        "        'bias':False,           # attention bias\n",
        "        'embed_type': 'position'# {'position','learned'}\n",
        "\n",
        "    },\n",
        "    'encoder':{\n",
        "        'type': 'swin_transformer_v2', # {'swin_transformer','swin_transformer_v2','resnet18','resnet50,'vgg'}\n",
        "        'swin':{\n",
        "            'img_size':(64,256),\n",
        "            'embed_dim':96,\n",
        "            'window_size':8,\n",
        "            'in_channels':3,\n",
        "            'dropout':0.1,\n",
        "            'depths': [2,6,2],\n",
        "            'num_heads': [6,12,24]\n",
        "        },\n",
        "    },\n",
        "    \n",
        "    'device':device,\n",
        "    'lr':1e-4,\n",
        "    'scheduler': {    \n",
        "        'active':True,\n",
        "        'first_cycle_steps': 50,\n",
        "        'cycle_mult': 1,  \n",
        "        'max_lr': 6e-4,          \n",
        "        'min_lr': 5e-5,   \n",
        "        'warmup_steps': 10,      \n",
        "        'gamma': 0.9  \n",
        "    },\n",
        "    'dataloader':{\n",
        "        'num_workers':0,\n",
        "\n",
        "    },\n",
        "    'max_grad_norm': 0.25,\n",
        "    'batch_size':25,\n",
        "    'num_epochs':200,\n",
        "    'save_per_epochs':10,\n",
        "    'print_type': 'per_batch' # {'per_epoch','per_batch'}\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "SRC_PATH = \"test/origin_200\"\n",
        "TARGET_PATH = \"test/train_gt.txt\"\n",
        "MODEL_PATH = \"model_weights/model_swin_transformer_v2_150.pt\"\n",
        "\n",
        "# SRC_PATH = \"/kaggle/working/new_train\"\n",
        "# TARGET_PATH = \"/kaggle/working/train_gt.txt\"\n",
        "# MODEL_PATH = \"./data\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "RVFVAU3JDvSV"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TRAINING CONTINUE!\n"
          ]
        }
      ],
      "source": [
        "trainer = Trainer(config      = config,\n",
        "                  IMAGE_PATH  = SRC_PATH,\n",
        "                  TARGET_PATH = TARGET_PATH,\n",
        "                  MODEL_PATH = MODEL_PATH)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total parameters: 11776738\n"
          ]
        }
      ],
      "source": [
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(f'Total parameters: {count_parameters(trainer.model)}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mTI7IzybF5Tp",
        "outputId": "e68155b7-7b07-4c4d-db15-5b70c9c41711"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "| Epoch   1 / 200: [###################]  100.00% | Loss: 3.2659 | Acc:   25.07% | ETA:     0.0s | \n",
            "| Epoch   2 / 200: [###################]  100.00% | Loss: 3.0775 | Acc:   28.80% | ETA:     0.0s | \n",
            "| Epoch   3 / 200: [###################]  100.00% | Loss: 2.8152 | Acc:   31.39% | ETA:     0.0s | \n",
            "| Epoch   4 / 200: [###################]  100.00% | Loss: 2.5590 | Acc:   35.02% | ETA:     0.0s | \n",
            "| Epoch   5 / 200: [###################]  100.00% | Loss: 2.4187 | Acc:   37.22% | ETA:     0.0s | \n",
            "| Epoch   6 / 200: [###################]  100.00% | Loss: 2.2952 | Acc:   41.72% | ETA:     0.0s | \n",
            "| Epoch   7 / 200: [###################]  100.00% | Loss: 2.2486 | Acc:   40.19% | ETA:     0.0s | \n",
            "| Epoch   8 / 200: [###################]  100.00% | Loss: 2.2277 | Acc:   41.24% | ETA:     0.0s | \n",
            "| Epoch   9 / 200: [###################]  100.00% | Loss: 2.1271 | Acc:   42.68% | ETA:     0.0s | \n",
            "| Epoch  10 / 200: [###################]  100.00% | Loss: 2.0199 | Acc:   45.45% | ETA:     0.0s | \n",
            "| Epoch  11 / 200: [###################]  100.00% | Loss: 1.9038 | Acc:   48.71% | ETA:     0.0s | \n",
            "| Epoch  12 / 200: [###################]  100.00% | Loss: 1.8257 | Acc:   50.53% | ETA:     0.0s | \n",
            "| Epoch  13 / 200: [###################]  100.00% | Loss: 1.7618 | Acc:   52.34% | ETA:     0.0s | \n",
            "| Epoch  14 / 200: [###################]  100.00% | Loss: 1.7488 | Acc:   52.15% | ETA:     0.0s | \n",
            "| Epoch  15 / 200: [###################]  100.00% | Loss: 1.7260 | Acc:   51.39% | ETA:     0.0s | \n",
            "| Epoch  16 / 200: [###################]  100.00% | Loss: 1.6762 | Acc:   53.97% | ETA:     0.0s | \n",
            "| Epoch  17 / 200: [###################]  100.00% | Loss: 1.5777 | Acc:   58.18% | ETA:     0.0s | \n",
            "| Epoch  18 / 200: [###################]  100.00% | Loss: 1.5201 | Acc:   58.47% | ETA:     0.0s | \n",
            "| Epoch  19 / 200: [###################]  100.00% | Loss: 1.4676 | Acc:   61.91% | ETA:     0.0s | \n",
            "| Epoch  20 / 200: [###################]  100.00% | Loss: 1.4691 | Acc:   59.52% | ETA:     0.0s | \n",
            "| Epoch  21 / 200: [###################]  100.00% | Loss: 1.4629 | Acc:   61.05% | ETA:     0.0s | \n",
            "| Epoch  22 / 200: [###################]  100.00% | Loss: 1.4404 | Acc:   60.38% | ETA:     0.0s | \n",
            "| Epoch  23 / 200: [###################]  100.00% | Loss: 1.3871 | Acc:   61.91% | ETA:     0.0s | \n",
            "| Epoch  24 / 200: [###################]  100.00% | Loss: 1.2872 | Acc:   66.03% | ETA:     0.0s | \n",
            "| Epoch  25 / 200: [###################]  100.00% | Loss: 1.2652 | Acc:   66.32% | ETA:     0.0s | \n",
            "| Epoch  26 / 200: [###################]  100.00% | Loss: 1.2448 | Acc:   68.80% | ETA:     0.0s | \n",
            "| Epoch  27 / 200: [###################]  100.00% | Loss: 1.2707 | Acc:   66.89% | ETA:     0.0s | \n",
            "| Epoch  28 / 200: [###################]  100.00% | Loss: 1.2494 | Acc:   65.45% | ETA:     0.0s | \n",
            "| Epoch  29 / 200: [###################]  100.00% | Loss: 1.2158 | Acc:   66.41% | ETA:     0.0s | \n",
            "| Epoch  30 / 200: [###################]  100.00% | Loss: 1.1635 | Acc:   67.37% | ETA:     0.0s | \n",
            "| Epoch  31 / 200: [###################]  100.00% | Loss: 1.1266 | Acc:   70.62% | ETA:     0.0s | \n",
            "| Epoch  32 / 200: [###################]  100.00% | Loss: 1.1031 | Acc:   70.81% | ETA:     0.0s | \n",
            "| Epoch  33 / 200: [###################]  100.00% | Loss: 1.0996 | Acc:   70.33% | ETA:     0.0s | \n",
            "| Epoch  34 / 200: [###################]  100.00% | Loss: 1.1386 | Acc:   68.23% | ETA:     0.0s | \n",
            "| Epoch  35 / 200: [###################]  100.00% | Loss: 1.1253 | Acc:   67.56% | ETA:     0.0s | \n",
            "| Epoch  36 / 200: [###################]  100.00% | Loss: 1.0770 | Acc:   70.05% | ETA:     0.0s | \n",
            "| Epoch  37 / 200: [###################]  100.00% | Loss: 1.0506 | Acc:   71.77% | ETA:     0.0s | \n",
            "| Epoch  38 / 200: [###################]  100.00% | Loss: 1.0116 | Acc:   72.63% | ETA:     0.0s | \n",
            "| Epoch  39 / 200: [###################]  100.00% | Loss: 1.0096 | Acc:   71.96% | ETA:     0.0s | \n",
            "| Epoch  40 / 200: [###################]  100.00% | Loss: 1.0253 | Acc:   71.48% | ETA:     0.0s | \n",
            "| Epoch  41 / 200: [###################]  100.00% | Loss: 0.9945 | Acc:   73.68% | ETA:     0.0s | \n",
            "| Epoch  42 / 200: [###################]  100.00% | Loss: 1.0030 | Acc:   72.92% | ETA:     0.0s | \n",
            "| Epoch  43 / 200: [###################]  100.00% | Loss: 0.9435 | Acc:   75.12% | ETA:     0.0s | \n",
            "| Epoch  44 / 200: [###################]  100.00% | Loss: 0.9324 | Acc:   74.55% | ETA:     0.0s | \n",
            "| Epoch  45 / 200: [###################]  100.00% | Loss: 0.9153 | Acc:   74.83% | ETA:     0.0s | \n",
            "| Epoch  46 / 200: [###################]  100.00% | Loss: 0.9437 | Acc:   73.40% | ETA:     0.0s | \n",
            "| Epoch  47 / 200: [###################]  100.00% | Loss: 0.9627 | Acc:   72.63% | ETA:     0.0s | \n",
            "| Epoch  48 / 200: [###################]  100.00% | Loss: 0.9476 | Acc:   74.45% | ETA:     0.0s | \n",
            "| Epoch  49 / 200: [###################]  100.00% | Loss: 0.9031 | Acc:   75.89% | ETA:     0.0s | \n",
            "| Epoch  50 / 200: [###################]  100.00% | Loss: 0.8736 | Acc:   76.08% | ETA:     0.0s | \n",
            "| Epoch  51 / 200: [###################]  100.00% | Loss: 0.8549 | Acc:   76.56% | ETA:     0.0s | \n",
            "| Epoch  52 / 200: [###################]  100.00% | Loss: 0.8811 | Acc:   75.02% | ETA:     0.0s | \n",
            "| Epoch  53 / 200: [###################]  100.00% | Loss: 0.9116 | Acc:   74.74% | ETA:     0.0s | \n",
            "| Epoch  54 / 200: [###################]  100.00% | Loss: 0.8874 | Acc:   74.35% | ETA:     0.0s | \n",
            "| Epoch  55 / 200: [###################]  100.00% | Loss: 0.8277 | Acc:   77.03% | ETA:     0.0s | \n",
            "| Epoch  56 / 200: [###################]  100.00% | Loss: 0.7918 | Acc:   79.43% | ETA:     0.0s | \n",
            "| Epoch  57 / 200: [###################]  100.00% | Loss: 0.8295 | Acc:   77.13% | ETA:     0.0s | \n",
            "| Epoch  58 / 200: [###################]  100.00% | Loss: 0.8242 | Acc:   77.61% | ETA:     0.0s | \n",
            "| Epoch  59 / 200: [###################]  100.00% | Loss: 0.8268 | Acc:   77.51% | ETA:     0.0s | \n",
            "| Epoch  60 / 200: [###################]  100.00% | Loss: 0.8101 | Acc:   76.56% | ETA:     0.0s | \n",
            "| Epoch  61 / 200: [###################]  100.00% | Loss: 0.8067 | Acc:   77.22% | ETA:     0.0s | \n",
            "| Epoch  62 / 200: [###################]  100.00% | Loss: 0.7796 | Acc:   79.14% | ETA:     0.0s | \n",
            "| Epoch  63 / 200: [###################]  100.00% | Loss: 0.7852 | Acc:   78.56% | ETA:     0.0s | \n",
            "| Epoch  64 / 200: [###################]  100.00% | Loss: 0.7485 | Acc:   80.00% | ETA:     0.0s | \n",
            "| Epoch  65 / 200: [###################]  100.00% | Loss: 0.7852 | Acc:   77.61% | ETA:     0.0s | \n",
            "| Epoch  66 / 200: [###################]  100.00% | Loss: 0.7706 | Acc:   78.66% | ETA:     0.0s | \n",
            "| Epoch  67 / 200: [###################]  100.00% | Loss: 0.7893 | Acc:   77.99% | ETA:     0.0s | \n",
            "| Epoch  68 / 200: [###################]  100.00% | Loss: 0.7435 | Acc:   79.04% | ETA:     0.0s | \n",
            "| Epoch  69 / 200: [###################]  100.00% | Loss: 0.7130 | Acc:   79.71% | ETA:     0.0s | \n",
            "| Epoch  70 / 200: [###################]  100.00% | Loss: 0.7328 | Acc:   80.19% | ETA:     0.0s | \n",
            "| Epoch  71 / 200: [###################]  100.00% | Loss: 0.7225 | Acc:   79.43% | ETA:     0.0s | \n",
            "| Epoch  72 / 200: [###################]  100.00% | Loss: 0.7537 | Acc:   78.56% | ETA:     0.0s | \n",
            "| Epoch  73 / 200: [###################]  100.00% | Loss: 0.7367 | Acc:   80.00% | ETA:     0.0s | \n",
            "| Epoch  74 / 200: [###################]  100.00% | Loss: 0.7389 | Acc:   79.71% | ETA:     0.0s | \n",
            "| Epoch  75 / 200: [###################]  100.00% | Loss: 0.6978 | Acc:   80.96% | ETA:     0.0s | \n",
            "| Epoch  76 / 200: [###################]  100.00% | Loss: 0.6946 | Acc:   80.67% | ETA:     0.0s | \n",
            "| Epoch  77 / 200: [###################]  100.00% | Loss: 0.6940 | Acc:   80.77% | ETA:     0.0s | \n",
            "| Epoch  78 / 200: [###################]  100.00% | Loss: 0.7104 | Acc:   80.10% | ETA:     0.0s | \n",
            "| Epoch  79 / 200: [###################]  100.00% | Loss: 0.7217 | Acc:   79.52% | ETA:     0.0s | \n",
            "| Epoch  80 / 200: [###################]  100.00% | Loss: 0.6907 | Acc:   79.71% | ETA:     0.0s | \n",
            "| Epoch  81 / 200: [###################]  100.00% | Loss: 0.6747 | Acc:   81.05% | ETA:     0.0s | \n",
            "| Epoch  82 / 200: [###################]  100.00% | Loss: 0.6636 | Acc:   81.72% | ETA:     0.0s | \n",
            "| Epoch  83 / 200: [###################]  100.00% | Loss: 0.6624 | Acc:   81.15% | ETA:     0.0s | \n",
            "| Epoch  84 / 200: [###################]  100.00% | Loss: 0.6769 | Acc:   82.11% | ETA:     0.0s | \n",
            "| Epoch  85 / 200: [###################]  100.00% | Loss: 0.6665 | Acc:   81.34% | ETA:     0.0s | \n",
            "| Epoch  86 / 200: [###################]  100.00% | Loss: 0.6440 | Acc:   81.34% | ETA:     0.0s | \n",
            "| Epoch  87 / 200: [###################]  100.00% | Loss: 0.6463 | Acc:   82.20% | ETA:     0.0s | \n",
            "| Epoch  88 / 200: [###################]  100.00% | Loss: 0.6193 | Acc:   82.78% | ETA:     0.0s | \n",
            "| Epoch  89 / 200: [###################]  100.00% | Loss: 0.6383 | Acc:   81.24% | ETA:     0.0s | \n",
            "| Epoch  90 / 200: [###################]  100.00% | Loss: 0.6255 | Acc:   82.11% | ETA:     0.0s | \n",
            "| Epoch  91 / 200: [###################]  100.00% | Loss: 0.6428 | Acc:   81.72% | ETA:     0.0s | \n",
            "| Epoch  92 / 200: [###################]  100.00% | Loss: 0.6051 | Acc:   84.31% | ETA:     0.0s | \n",
            "| Epoch  93 / 200: [#######------------]   37.50% | Loss: 0.5922 | Acc:   82.74% | ETA:     1.6s | "
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_13576/4032920361.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[1;32mc:\\Users\\vodin\\project-DD\\src\\model\\trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     78\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtarget_input\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_output\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_padding\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     79\u001b[0m                 \u001b[0mstart_time\u001b[0m     \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mperf_counter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 80\u001b[1;33m                 \u001b[0mlogits\u001b[0m         \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtarget_input\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# (B,L,V)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     81\u001b[0m                 \u001b[0mtarget_padding\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtarget_padding\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     82\u001b[0m                 \u001b[0mtarget_output\u001b[0m  \u001b[1;33m=\u001b[0m \u001b[0mtarget_output\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mc:\\Users\\vodin\\.conda\\envs\\pytorchenv\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mc:\\Users\\vodin\\project-DD\\src\\model\\model.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, src, target, tar_pad, mode)\u001b[0m\n\u001b[0;32m    192\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtarget\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtar_pad\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'train'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    193\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'train'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 194\u001b[1;33m             \u001b[0mencoder_out\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# (B,H/32 * W/32,C)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    195\u001b[0m             logits = self._forward_decoder(target   = target,\n\u001b[0;32m    196\u001b[0m                                         \u001b[0mencoder_out\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mencoder_out\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mc:\\Users\\vodin\\.conda\\envs\\pytorchenv\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mc:\\Users\\vodin\\project-DD\\src\\model\\model.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    127\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    128\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencoder_type\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'swin_transformer'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'swin_transformer_v2'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 129\u001b[1;33m             \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# (B, H/32 * W/32, C)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    130\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    131\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mc:\\Users\\vodin\\.conda\\envs\\pytorchenv\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mc:\\Users\\vodin\\project-DD\\src\\backbone\\swin_transformer_v2.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    805\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    806\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 807\u001b[1;33m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward_features\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    808\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mc:\\Users\\vodin\\project-DD\\src\\backbone\\swin_transformer_v2.py\u001b[0m in \u001b[0;36mforward_features\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    801\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    802\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 803\u001b[1;33m             \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    804\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    805\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mc:\\Users\\vodin\\.conda\\envs\\pytorchenv\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mc:\\Users\\vodin\\project-DD\\src\\backbone\\swin_transformer_v2.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    638\u001b[0m                 \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheckpoint\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mblk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    639\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 640\u001b[1;33m                 \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mblk\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    641\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdownsample\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    642\u001b[0m             \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdownsample\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mc:\\Users\\vodin\\.conda\\envs\\pytorchenv\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mc:\\Users\\vodin\\project-DD\\src\\backbone\\swin_transformer_v2.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    518\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    519\u001b[0m         \u001b[1;31m# W-MSA/SW-MSA\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 520\u001b[1;33m         \u001b[0mattn_windows\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mattn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_windows\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mattn_mask\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# nW*B, window_size*window_size, C\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    521\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    522\u001b[0m         \u001b[1;31m# merge windows\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mc:\\Users\\vodin\\.conda\\envs\\pytorchenv\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mc:\\Users\\vodin\\project-DD\\src\\backbone\\swin_transformer_v2.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x, mask)\u001b[0m\n\u001b[0;32m    398\u001b[0m         \u001b[1;31m# cosine attention\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    399\u001b[0m         \u001b[0mattn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnormalize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mq\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m@\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnormalize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 400\u001b[1;33m         \u001b[0mlogit_scale\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclamp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlogit_scale\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1.\u001b[0m \u001b[1;33m/\u001b[0m \u001b[1;36m0.01\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    401\u001b[0m         \u001b[0mattn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mattn\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mlogit_scale\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    402\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "trainer.train()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "pytorchenv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.16"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
